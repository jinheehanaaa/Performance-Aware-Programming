
<!-- AFTER 30MIN -->
<details>
    <summary>0 ~ 39</summary>

# Register Renaming (0 ~ 10)
- We will talk about this later
# Python over C++ (10 ~ 13)  
- Quick, Not care about where things are going
# Performance & Hardware (13 ~ 19)
- Performance-aware programming vs. Optimization
# Compiler & CPU Designer (19 ~ 21)
# Real Code practice (21 ~ 23)
# Figure out waste for entire code (22 ~ 26)
- "How often is our CPU doing that "ADD" as opposed to everything else is doing" => leads waste problem
- Profiler
# Optimization (26 ~ 28)
- Think about limiting factors
- - EX: "CPU/sec" "bytes/sec" "what kind of cache" 
# Focusing on Asymptotic performance (28 ~ 29)
- O(N), O(N^2), O(NLogN), etc are important
# Performance (30 ~ 33)

</details>


<!-- AFTER 1HR -->
<details>
    <summary>30 ~ 59</summary>

# SIMD Signed/Unsigned (33 ~ 44)
- Twos complement
- m128i
- _mm_add_eqi32

# SIMD & Compiler (44 ~ 47)
- Auto-vectorization for some compiler (It's case by case)

# Intel & AMD for SIMD (47 ~ 50)
- Same
 
# CPU (50 ~ 56)
- Register
- - Scaler Value: 64 bits
- - SIMD (or Vector) Value: 256 or 512 bits

# SIMD vs. GPU (56 ~ 1:04)
- GPU is another SIMD CPU in your machine
- CPU => High Clock, High IPC
- GPU => Low Clock, More ALUs, More Queues, More Hyperthread
- GPU is more SIMD unit than CPU

</details>



<details>
    <summary>1:00 ~ 1:30</summary>

# Deterministic CPU (1:04 ~ 1:09)
- To maximize performance
 
# SSE & AVX (1:09 ~ 1:11)

# ARM CPU (1:11 ~ 1:15)
- ARM CPU is much more basic

# SIMD 32 BIT & Overflow (1:15 ~ 1:19)

# SSE & Clock Speed (1:19 - 1:21)

# SIMD & Hardware Compatibility (1:21 ~ 1:24)
- Not every PC has AVX512 support
- Use CPU ID let CPU tell what instruction they have

# Debug for SIMD (1:24 ~ 1:35)
- Only Debugging feature for CPU is assembly langauge
- CPU & Out-of-Order Operation
- Stall for Memory fence instruction

</details>



<details>
    <summary>1:30 ~ 2:00</summary>

# SIMD Array (1:35 ~ 1:43)
- "SHOT" "Tail"
- PAD
- Masking
- PACKED Instruction sets
- VECTOR Instruction sets
- Avoid Scalor loop if you can

# SIMD Circuit (1:43 ~ )

# ??? (1:44 ~) 

# Overhead using SIMD (1:45 - 1:50)
- no overhead but tends to be more latant
- Latency: How long it takes to compute
- Throughput: 

# Load Unit (1:50 ~ 1:54)

# CPU Design (1:54 ~ 2:01)
- Register file & Lane


</details>

<!-- CACHING -->
<details>
    <summary>2:00 ~ 2:30</summary>

# Controlling the Cache (2:01 ~ 2:10)
- You don't get very much for cache control
- You can give hints for what kind of cache you want to do 
- - 1. Pre-fetch instruction (Bring the value into the cache)
- - 2. Streaming instruction (Not going to read again, so don't store in L1, L2, or L3)
- - - - Streaming Load: 
- - - - Streamng Store: 
  
# Data Flow in Cache (2:10 ~ 2:19)
# Pipeline 
- Main Memory to ALU => Can't use whole L1 bandwidth (Maximum rate of speed comes from Main Memoray)
- L1 to ALU => Can use whole L1 bandwidth (Maximum rate of speed comes from L1)
- It's not just taking  more cycles but also amount of data we get in byte is much lower in main memory 

# Hardware Prefetching (2:19 ~ 2:24)
- Eliminates latency by starting it early (but doesn't increase the throughput)
- Throughput: # of byttes per cycle maximum I can receive (b/c it is restrcted by pipeline) 

# Cache Pollution (2:24 ~ 2:28)

# Speed & Size for Pipeline (2:28 ~ 2:29)
- Bytes in each memory are not 2^n (EX: 64, 20, 12, 5.1)

# Scalar Add (2:29 ~ 2:31)

# Plot (2:31 ~ 2:34)

# 64 bytes Cacheline (2:34 ~ 2:40)
- 2 Cacheline & Weldng opeartion
- 4096 Page Boundary Penalty

# Cache Penalty (2:40 ~ 2:41)
- No extra penalty for checking

# Cache & CPU (2:41 ~ 2:43)

# Cache (2:43 ~ 2:46)
- Minimze # of time of read & write pull

# Bigger L1 Cache (2:46 - 2:48)
- Can't have enough space
- We can't physically pack 

# Instructions in Cache? (2:48 ~ 2:52)
- Only L1 has 2 part of cache
- - Seprate Cache for Instruction
- - Seprate Cache for Data
- Rest stuffs are all the same

# Primed Cache (2:52 ~ 2:53)
- Look for peak throughput & take the fastest run
- You expect the same aspect per cycle (In our case: Just looking at peak add per cycle)
- Real program might be different

# L1 Cache Overlap (2:53 ~ 2:55)

# Debug (2:55 ~ 2:56)
- We will talk about this later

# Prefetcher (2:56 ~ 3:00)

# OS Memory Map (3:00 ~ 2:01)

# Cache (3:01 ~ 3:03)
- Fill slots in order (L1 -> L2 -> L3 -> Main Memory)
- CPU never thinks like "I'm going to move something from L1 to L2" 
- - This is DMA based system => It's not caching anymore

# Quad Pointer (3:03 - 3:05)

# CPU & GPU (3:05 ~ 3:06) 

# Performance (3:07 ~ 3:09)
- Adds / cycle = Total Adds / Total Cycle (Work Size/ Cycle)

# Optimization (3:09 ~ 3:10)

# Loop (3:10 ~ 3:11)

# Cache (3:12 ~ 3:14)
- Inclusive caches: Includes copy
- exclusive caches

# L4, L5 Cache? (3:14 ~ )
- There are not much differences in speed with having L4, L5 Cache

</details>

<!-- AFTER 3HR -->
<details>
    <summary>2:30 ~ 3:16</summary>

</details>


# Resources
